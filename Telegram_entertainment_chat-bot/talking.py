import emoji
import nltk
import re
import string
from datetime import datetime
from pymorphy3 import MorphAnalyzer
from stop_words import get_stop_words
from transformers import AutoModelForCausalLM, AutoTokenizer
nltk.download('stopwords')
# nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')

model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
txtgen_model = AutoModelForCausalLM.from_pretrained(model_name)

MorphAnalyzer = MorphAnalyzer()
sw = set(get_stop_words('ru') + nltk.corpus.stopwords.words('russian'))
exclude = set(string.punctuation)

history_dialog = []
histories = {}


def preprocess_txt(txt):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è –∑–∞–¥–∞—á–∏ NLP.

    –§—É–Ω–∫—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞:
    1. –£–¥–∞–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, HTML-—Ç–µ–≥–∏, URL-–∞–¥—Ä–µ—Å–∞ –∏ –Ω–µ–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã.
    2. –ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.
    3. –£–¥–∞–ª—è–µ—Ç —ç–º–æ–¥–∑–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞.
    4. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç.
    5. –£–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
    6. –í—ã–ø–æ–ª–Ω—è–µ—Ç –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Å–ª–æ–≤.

    Args:
        txt (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.

    Returns:
        txt (list): –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤) –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ,
        –æ—á–∏—â–µ–Ω–Ω—ã—Ö –æ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.
    """
    txt = re.sub(r'[^\w\s]', ' ', txt)
    txt = re.sub(r'http\S+', '', txt)
    txt = re.sub('<[^<]+?>', '', txt)
    txt = re.sub(r'[^–∞-—è–ê-–Ø]', ' ', txt)
    txt = txt.lower()
    txt = emoji.replace_emoji(txt, replace='')
    txt = ' '.join([w for w in txt.split() if len(w) > 1])
    txt = nltk.tokenize.word_tokenize(txt)
    txt = [item for item in txt if item not in nltk.corpus.stopwords.words('russian')]
    txt = [nltk.stem.wordnet.WordNetLemmatizer().lemmatize(word) for word in txt]
    return txt


def respond_to_dialog(texts):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.

    –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –µ—ë,
    –ø–µ—Ä–µ–¥–∞—ë—Ç –≤ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å 'sberbank-ai/rugpt3small_based_on_gpt2' –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç
    —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.

    Args:
        texts (str): –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫, –≥–¥–µ —ç–ª–µ–º–µ–Ω—Ç—ã —á–µ—Ä–µ–¥—É—é—Ç—Å—è:
            - –ß—ë—Ç–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã - —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            - –ù–µ—á—ë—Ç–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã - –æ—Ç–≤–µ—Ç—ã –±–æ—Ç–∞.

    Returns:
        result (str): –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –±–æ—Ç–∞.
    """
    prefix = '\nx:'
    for i, t in enumerate(texts):
        prefix += t
        prefix += '\nx:' if i % 2 == 1 else '\ny:'
    tokens = tokenizer(prefix, return_tensors='pt')
    tokens = {k: v.to(txtgen_model.device) for k, v in tokens.items()}
    end_token_id = tokenizer.encode('\n')[0]
    size = tokens['input_ids'].shape[1]
    output = txtgen_model.generate(
        **tokens,
        eos_token_id=end_token_id,
        do_sample=True,
        max_length=size + 128,
        repetition_penalty=3.2,
        temperature=1,
        num_beams=3,
        length_penalty=0.01,
        pad_token_id=tokenizer.eos_token_id
    )
    decoded = tokenizer.decode(output[0])
    result = re.findall(r'\ny:(.+)', decoded)[-1]

    return result.strip()


def conversation(txt, user_id=None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.

    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–µ –æ—Ç–≤–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é
    —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥.

    Args:
        txt (str): –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ.
        user_id (str): –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ
        –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.

    Returns:
        response (str): –û—Ç–≤–µ—Ç –±–æ—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞.
        –û—Ç–≤–µ—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Å —É—á—ë—Ç–æ–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    global history_dialog

    if user_id not in histories:
        histories[user_id] = []

    history_dialog = histories[user_id]
    history_dialog.append(txt)
    context = history_dialog[-100:]

    response = respond_to_dialog(context)
    history_dialog.append(response)

    log_file = f'logs/conversations/user_{user_id}.txt'
    timestamp = datetime.now().strftime('%d.%m.%Y %H:%M:%S')

    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n[{timestamp}] üë§: {txt}\n")
        f.write(f"[{timestamp}] ü§ñ: {response}\n")
        f.write(f"{'-' * 60}\n")

    return response


if __name__ == '__main__':
    dialog = input("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç\n")
    dialog = conversation(dialog)
    print(dialog)
